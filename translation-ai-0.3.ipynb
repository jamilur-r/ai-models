{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3ec234-73c3-4b3c-b10a-6323ff76ad45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.20.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.96)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3761472-0222-4c13-b42d-2a7bf39a1baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses) (1.1.0)\n",
      "Requirement already satisfied: regex in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->sacremoses) (0.4.5)\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17007473-9026-4dfd-a530-1c495ed35490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87169ba-a879-4eb7-9b14-68a1e679ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32b2f9eb-ea7e-474e-8b96-91400d457386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = \"en\"\n",
    "dst = \"de\"\n",
    "\n",
    "task_name = f\"translation_{src}_to_{dst}\"\n",
    "# model_name = f\"Helsinki-NLP/opus-mt-{src}-{dst}\"\n",
    "model_name = 'Helsinki-NLP/opus-mt-th-en'\n",
    "\n",
    "translator  = pipeline(task_name, model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92c89ee0-1338-4a54-9af5-3577a474d1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ฉันชื่ออาบีร์\n",
      "Actual: My name is abir\n",
      "Predicted: My name is Abie.\n",
      "Text: ฉันเป็นนักพัฒนา\n",
      "Actual: I am a developer\n",
      "Predicted: I'm a developer.\n",
      "Text: ฉันชอบเขียนโค้ดในจาวาสคริปต์\n",
      "Actual: I love coding in javascript\n",
      "Predicted: I love writing code in JavaScript.\n",
      "Text: เดทตอล ดิสอินเฟคแทนท์ ไวพ์ส กลิ่น เฟรช 45 แผ่น x6\n",
      "Actual: Dettol Disinfectant Wipes Fresh scent 45 sheets x6 Dettol Disinfectant Wipes 45 sheets x 6\n",
      "Predicted: Dead Death Disfectant, Wipps Smells 45 Fresh x6.\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"ฉันชื่ออาบีร์\",\n",
    "    \"ฉันเป็นนักพัฒนา\",\n",
    "    \"ฉันชอบเขียนโค้ดในจาวาสคริปต์\",\n",
    "    \"เดทตอล ดิสอินเฟคแทนท์ ไวพ์ส กลิ่น เฟรช 45 แผ่น x6\"\n",
    "]\n",
    "actuals = [\n",
    "    \"My name is abir\",\n",
    "    \"I am a developer\",\n",
    "    \"I love coding in javascript\",\n",
    "    \"Dettol Disinfectant Wipes Fresh scent 45 sheets x6 Dettol Disinfectant Wipes 45 sheets x 6\"\n",
    "]\n",
    "for i in range(len(texts)):\n",
    "    text = texts[i]\n",
    "    actual = actuals[i]\n",
    "    predict = translator(text)[0][\"translation_text\"]  \n",
    "    \n",
    "    print(f'Text: {text}')\n",
    "    print(f'Actual: {actual}')\n",
    "    print(f'Predicted: {predict}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "724215b5-0b76-4fc7-9661-a80be271f1ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.20.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n",
      "     ------------------------------------ 362.3/362.3 KB 661.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (4.64.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (1.23.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (0.8.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (0.12.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (2022.7.9)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (3.19.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]) (0.1.96)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp310-cp310-win_amd64.whl (555 kB)\n",
      "     ------------------------------------ 555.1/555.1 KB 396.2 kB/s eta 0:00:00\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n",
      "     ------------------------------------ 133.1/133.1 KB 218.5 kB/s eta 0:00:00\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.4.3)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "     ------------------------------------ 140.6/140.6 KB 363.2 kB/s eta 0:00:00\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-8.0.0-cp310-cp310-win_amd64.whl (17.9 MB)\n",
      "     -------------------------------------- 17.9/17.9 MB 314.2 kB/s eta 0:00:00\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "     -------------------------------------- 95.8/95.8 KB 195.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[sentencepiece]) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[sentencepiece]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[sentencepiece]) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[sentencepiece]) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers[sentencepiece]) (0.4.5)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp310-cp310-win_amd64.whl (33 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp310-cp310-win_amd64.whl (122 kB)\n",
      "     ------------------------------------ 122.2/122.2 KB 287.0 kB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp310-cp310-win_amd64.whl (27 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.3.2 dill-0.3.5.1 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 multiprocess-0.70.13 pyarrow-8.0.0 responses-0.18.0 xxhash-3.0.0 yarl-1.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n",
      "     -------------------------------------- 92.0/92.0 KB 879.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.96)\n",
      "Requirement already satisfied: regex in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu) (2022.7.9)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu) (1.23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacrebleu) (0.4.5)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from portalocker->sacrebleu) (304)\n",
      "Installing collected packages: tabulate, portalocker, sacrebleu\n",
      "Successfully installed portalocker-2.5.1 sacrebleu-2.1.0 tabulate-0.8.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (4.64.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->huggingface_hub) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers[sentencepiece] datasets\n",
    "! pip install sacrebleu sentencepiece\n",
    "! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37210f1a-edb5-4b8b-b1a0-d7dc64910c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.20.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96134c24-5aad-4a2e-8373-e0433c50d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'Helsinki-NLP/opus-mt-th-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df0947-074f-46db-a983-57979bcafd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d798b-9812-4c1f-8d1a-48aefa6d3437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb07f64f-9ba1-458d-a8c8-637c9e3b6279",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9ccabcb213ae7f13\n",
      "Reusing dataset csv (C:\\Users\\jamilur\\.cache\\huggingface\\datasets\\csv\\default-9ccabcb213ae7f13\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 199.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset('csv', data_files='translation-dataset/generated_reviews_crowd.csv')\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b64305ac-1b64-4ba0-8a7d-b17046f0700c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en_text', 'th_text'],\n",
       "        num_rows: 24587\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "348e3802-f929-4bf9-90c8-ebcd2f8c4347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_text': \"Doesn't snap together well.\", 'th_text': 'เข้ากันไม่ค่อยดี'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "386cf6c0-7543-472a-aea4-a76587158e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_text</th>\n",
       "      <th>th_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do Not buy this product, they are not real books.</td>\n",
       "      <td>อย่าซื้อสินค้านี้ พวกเขาไม่ใช้หนังสือจริงๆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i WILL NEVER BUY IT AGAIN.</td>\n",
       "      <td>ฉันจะไม่ซื้ออีกเลย</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It only has two episodes on each disc with no extras and those aren't even that great.</td>\n",
       "      <td>มันมีแค่สองตอนในแต่ละแผ่นก้วยไม่มีเพิ่มและพวกนั้นไม่ได้ดีนัก</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The streaming quality is great.</td>\n",
       "      <td>คุณาพสตรีมมิ่งดีเยี่ยม</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not enough room and she never used it once.</td>\n",
       "      <td>มีห้องไม่พอ และเธอไม่เคยใช้แม้แต่ครั้งเดียว</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "    \n",
    "show_random_elements(raw_datasets[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e46ed6a0-3dce-41a4-8bf1-00030506a61d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2bcbcc8-e0d4-493a-9b9a-aa40a6a325af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [0, 0, 0, 0],\n",
       " 'totals': [19, 17, 15, 13],\n",
       " 'precisions': [0.0, 0.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 19,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"Do Not buy this product, they are not real books.\", \"i WILL NEVER BUY IT AGAIN.\"]\n",
    "fake_labels = [[\"อย่าซื้อสินค้านี้ พวกเขาไม่ใช้หนังสือจริงๆ+\"], [\"ฉันจะไม่ซื้ออีกเลย\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a52323c9-9060-4f02-a4ea-7e19e24da017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b444f0d6-bbcc-4e09-8c08-635cb18d9e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3613, 1637, 8992, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "53d927bc-b559-43fd-b91e-b43aa1e39a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"th_text\"\n",
    "target_lang = \"en_text\"\n",
    "prefix = \"translate Thai to English: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example for example in examples[source_lang]]\n",
    "    targets = [example for example in examples[target_lang]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "07d712f9-9d95-4140-8455-45aa2dbce17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:04<00:00,  5.51ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en_text', 'th_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 24587\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2f12350a-5048-4365-9cf5-c01be9ae0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bd11f149-b353-4ad5-b059-f1accc4dbe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the weights of TFMarianMTModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2764c51b-1a28-4b5e-a90d-4ff9804dd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "push_to_hub_model_id = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "37dcc677-ce49-46a3-985c-e77f7c9c209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6f2be5b4-9b64-451b-8deb-cceb548487b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_data[\"train\"].to_tf_dataset(\n",
    "    batch_size=batch_size,\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "validation_dataset = tokenized_data[\"train\"].to_tf_dataset(\n",
    "    batch_size=batch_size,\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "generation_dataset = (\n",
    "    tokenized_data[\"train\"]\n",
    "    .shuffle()\n",
    "    .select(list(range(48)))\n",
    "    .to_tf_dataset(\n",
    "        batch_size=8,\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1e35c27-9169-4467-a9f7-d199f5b3caba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "import tensorflow as tf\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "81dde81c-3d1a-4126-849c-60e4861c2cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No label_cols specified for KerasMetricCallback, assuming you want the 'labels' key.\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    preds, labels = eval_predictions\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # We use -100 to mask labels - replace it with the tokenizer pad token when decoding\n",
    "    # so that no output is emitted for these\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result\n",
    "\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ad1ee-47fb-43ff-97e6-02a4964b84e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  75/1537 [>.............................] - ETA: 1:06:47 - loss: 1.9634"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n",
    "\n",
    "\n",
    "\n",
    "# callbacks = [tensorboard_callback, metric_callback, push_to_hub_callback]\n",
    "callbacks = [metric_callback, tensorboard_callback, ]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc71307-85aa-4e90-88b3-d383c4ba4bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dbb594-1a8c-4dc1-97e6-3acd041d9861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
