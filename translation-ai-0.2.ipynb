{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbed0900-b6a2-4348-a31f-be60158b5e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: seaborn in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: keras in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: click in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (1.23.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (1.4.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.34.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk seaborn keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61c385df-9bf1-491e-b8cc-76225ebba098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e95850b-cddd-4650-a0d0-6643ac4df10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_text</th>\n",
       "      <th>th_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doesn't snap together well.</td>\n",
       "      <td>เข้ากันไม่ค่อยดี</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charged it after every use as directed for abo...</td>\n",
       "      <td>เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My son wanted this movie so badly, that he sai...</td>\n",
       "      <td>ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But his writing has degenerated in later books.</td>\n",
       "      <td>แต่หนังสือเล่มที่ผ่านมาของเขามันดูด้อยคุณภาพลง</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was supposed to be a good bag, well you get...</td>\n",
       "      <td>มันควรที่จะเป็นกระเป๋าที่ดี เอ้อ เธอได้ในสิ่งท...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_text  \\\n",
       "0                        Doesn't snap together well.   \n",
       "1  Charged it after every use as directed for abo...   \n",
       "2  My son wanted this movie so badly, that he sai...   \n",
       "3    But his writing has degenerated in later books.   \n",
       "4  It was supposed to be a good bag, well you get...   \n",
       "\n",
       "                                             th_text  \n",
       "0                                   เข้ากันไม่ค่อยดี  \n",
       "1  เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไ...  \n",
       "2  ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี...  \n",
       "3     แต่หนังสือเล่มที่ผ่านมาของเขามันดูด้อยคุณภาพลง  \n",
       "4  มันควรที่จะเป็นกระเป๋าที่ดี เอ้อ เธอได้ในสิ่งท...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('translation-dataset/generated_reviews_crowd.csv',)\n",
    "\n",
    "df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de0bf0e7-b372-4a9d-870d-bbe9358af1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values before dropping duplicates\n",
      "24587\n",
      "24585\n",
      "\n",
      "\n",
      "Unique values before dropping duplicates\n",
      "24585\n",
      "24585\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Unique values before dropping duplicates\")\n",
    "print(df.en_text.nunique())\n",
    "print(df.th_text.nunique())\n",
    "\n",
    "df.drop_duplicates(subset=['en_text'],inplace=True)\n",
    "df.drop_duplicates(subset=['th_text'],inplace=True)\n",
    "\n",
    "print(\"\\n\\nUnique values before dropping duplicates\")\n",
    "print(df.th_text.nunique())\n",
    "print(df.th_text.nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28cf6be9-c72f-4038-8911-abf2d972e41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_text    False\n",
      "th_text    False\n",
      "dtype: bool \n",
      "\n",
      "en_text    0\n",
      "th_text    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().any(),'\\n')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "046b9933-2670-467c-a0e3-71921a6402f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jamilur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "#language => either 'english' or 'french'\n",
    "def preprocess(sentence,language):\n",
    "    sentence = sentence.lower()\n",
    "    if language == \"english\":\n",
    "        sentence = ' '.join([contractions[word] if word in contractions else word for word in sentence.split()])\n",
    "#         sentence = ' '.join([word for word in sentence.split() if word not in eng_stopwords])\n",
    "    sentence = re.sub(r\"[.'!#$%&\\'()*+,-./:;<=>?@[\\\\]^ `{|}~]\",\" \",sentence)\n",
    "    sentence = ' '.join([word for word in sentence.split()])\n",
    "    \n",
    "    return sentence\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "959e74df-192c-470a-89a3-d185d3fd1b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24585, 2) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24585 entries, 0 to 24586\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   en_text  24585 non-null  object\n",
      " 1   th_text  24585 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 576.2+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.th_text = df.th_text.apply(lambda x:preprocess(x,'th_text'))\n",
    "df.en_text = df.en_text.apply(lambda x:preprocess(x,'en_text'))\n",
    "\n",
    "print(df.shape,'\\n')\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1f274a2-89ee-45c8-bb45-5a7b287a90af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_text</th>\n",
       "      <th>th_text</th>\n",
       "      <th>en_input</th>\n",
       "      <th>en_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doesn't snap together well.</td>\n",
       "      <td>เข้ากันไม่ค่อยดี</td>\n",
       "      <td>sostoken doesn't snap together well.</td>\n",
       "      <td>doesn't snap together well. eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>charged it after every use as directed for abo...</td>\n",
       "      <td>เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไ...</td>\n",
       "      <td>sostoken charged it after every use as directe...</td>\n",
       "      <td>charged it after every use as directed for abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my son wanted this movie so badly, that he sai...</td>\n",
       "      <td>ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี...</td>\n",
       "      <td>sostoken my son wanted this movie so badly, th...</td>\n",
       "      <td>my son wanted this movie so badly, that he sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>but his writing has degenerated in later books.</td>\n",
       "      <td>แต่หนังสือเล่มที่ผ่านมาของเขามันดูด้อยคุณภาพลง</td>\n",
       "      <td>sostoken but his writing has degenerated in la...</td>\n",
       "      <td>but his writing has degenerated in later books...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it was supposed to be a good bag, well you get...</td>\n",
       "      <td>มันควรที่จะเป็นกระเป๋าที่ดี เอ้อ เธอได้ในสิ่งท...</td>\n",
       "      <td>sostoken it was supposed to be a good bag, wel...</td>\n",
       "      <td>it was supposed to be a good bag, well you get...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_text  \\\n",
       "0                        doesn't snap together well.   \n",
       "1  charged it after every use as directed for abo...   \n",
       "2  my son wanted this movie so badly, that he sai...   \n",
       "3    but his writing has degenerated in later books.   \n",
       "4  it was supposed to be a good bag, well you get...   \n",
       "\n",
       "                                             th_text  \\\n",
       "0                                   เข้ากันไม่ค่อยดี   \n",
       "1  เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไ...   \n",
       "2  ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี...   \n",
       "3     แต่หนังสือเล่มที่ผ่านมาของเขามันดูด้อยคุณภาพลง   \n",
       "4  มันควรที่จะเป็นกระเป๋าที่ดี เอ้อ เธอได้ในสิ่งท...   \n",
       "\n",
       "                                            en_input  \\\n",
       "0               sostoken doesn't snap together well.   \n",
       "1  sostoken charged it after every use as directe...   \n",
       "2  sostoken my son wanted this movie so badly, th...   \n",
       "3  sostoken but his writing has degenerated in la...   \n",
       "4  sostoken it was supposed to be a good bag, wel...   \n",
       "\n",
       "                                            en_label  \n",
       "0               doesn't snap together well. eostoken  \n",
       "1  charged it after every use as directed for abo...  \n",
       "2  my son wanted this movie so badly, that he sai...  \n",
       "3  but his writing has degenerated in later books...  \n",
       "4  it was supposed to be a good bag, well you get...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "df[\"en_input\"] = df.en_text.apply(lambda x:'sostoken ' + x)\n",
    "df[\"en_label\"] = df.en_text.apply(lambda x:x + ' eostoken')\n",
    "\n",
    "encoder_input = np.array(df.th_text)\n",
    "decoder_input = np.array(df.en_input)\n",
    "decoder_label = np.array(df.en_label)\n",
    "\n",
    "\n",
    "indices = np.arange(24585)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_label = decoder_label[indices]\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72ed3164-f153-4fad-b85e-6e7173caa46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape\n",
      "(17210,)\n",
      "(17210,)\n",
      "(17210,)\n",
      "\n",
      "\n",
      "test dataset shape\n",
      "(7375,)\n",
      "(7375,)\n",
      "(7375,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "total = df.shape[0]\n",
    "test_size = 0.3\n",
    "\n",
    "train_encoder_input = encoder_input[:-int(total*test_size)]\n",
    "train_decoder_input = decoder_input[:-int(total*test_size)]\n",
    "train_decoder_label = decoder_label[:-int(total*test_size)]\n",
    "\n",
    "test_encoder_input = encoder_input[-int(total*test_size):]\n",
    "test_decoder_input = decoder_input[-int(total*test_size):]\n",
    "test_decoder_label = decoder_label[-int(total*test_size):]\n",
    "\n",
    "print(\"train dataset shape\")\n",
    "print(train_encoder_input.shape)\n",
    "print(train_decoder_input.shape)\n",
    "print(train_decoder_label.shape)\n",
    "\n",
    "print(\"\\n\\ntest dataset shape\")\n",
    "print(test_encoder_input.shape)\n",
    "print(test_decoder_input.shape)\n",
    "print(test_decoder_label.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "962cc064-c731-4121-b046-de1d4b68adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words used in english sentences >> 32226\n",
      "Number of unique words used in french sentences >> 10201\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "thai_tok = Tokenizer()\n",
    "thai_tok.fit_on_texts(train_encoder_input)\n",
    "print(f\"Number of unique words used in english sentences >> {len(thai_tok.index_word)}\")\n",
    "\n",
    "en_tok = Tokenizer()\n",
    "en_tok.fit_on_texts(train_decoder_input)\n",
    "en_tok.fit_on_texts(train_decoder_label)\n",
    "print(f\"Number of unique words used in french sentences >> {len(en_tok.index_word)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b33adefd-dd33-4b19-bbbf-1b94d1acd1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= english =========================\n",
      "4475 of 10201 words are used less than 3times,\n",
      "which is only 44.0% of total words used\n",
      "But they occupy 2.0% of total frequency \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "total_counts = 0\n",
    "rare_counts = 0\n",
    "total_freq = 0\n",
    "rare_freq = 0\n",
    "\n",
    "least_occurence = 3\n",
    "for k,v in en_tok.word_counts.items():\n",
    "    total_counts +=1\n",
    "    total_freq += v\n",
    "    if v < least_occurence:\n",
    "        rare_counts+=1\n",
    "        rare_freq += v\n",
    "\n",
    "print(\"=\"*25,\"english\",\"=\"*25)\n",
    "print(f\"{rare_counts} of {total_counts} words are used less than {least_occurence}times,\")\n",
    "print(f\"which is only {np.round(rare_counts/total_counts*100)}% of total words used\")\n",
    "print(f\"But they occupy {np.round(rare_freq/total_freq*100)}% of total frequency \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7e2bc0f-64e6-4bd6-abd2-ea115747e512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= TH =========================\n",
      "31746 of 32226 words are used less than 3times,\n",
      "which is only 99.0% of total words used\n",
      "But they occupy 84.0% of total frequency \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "total_counts = 0\n",
    "rare_counts = 0\n",
    "total_freq = 0\n",
    "rare_freq = 0\n",
    "\n",
    "least_occurence = 3\n",
    "for k,v in thai_tok.word_counts.items():\n",
    "    total_counts +=1\n",
    "    total_freq += v\n",
    "    if v < least_occurence:\n",
    "        rare_counts+=1\n",
    "        rare_freq += v\n",
    "\n",
    "print(\"=\"*25,\"TH\",\"=\"*25)\n",
    "print(f\"{rare_counts} of {total_counts} words are used less than {least_occurence}times,\")\n",
    "print(f\"which is only {np.round(rare_counts/total_counts*100)}% of total words used\")\n",
    "print(f\"But they occupy {np.round(rare_freq/total_freq*100)}% of total frequency \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "110c319b-52ba-4836-9cc8-5d90edfcc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "thai_word_size = 6000\n",
    "thai_vocab_size = thai_word_size+1\n",
    "en_word_size = 12000\n",
    "en_vocab_size = en_word_size+1\n",
    "\n",
    "thai_tok = Tokenizer(num_words=thai_word_size)\n",
    "thai_tok.fit_on_texts(train_encoder_input)\n",
    "\n",
    "train_encoder_input = thai_tok.texts_to_sequences(train_encoder_input)\n",
    "test_encoder_input = thai_tok.texts_to_sequences(test_encoder_input)\n",
    "\n",
    "en_tok = Tokenizer(num_words=en_word_size)\n",
    "en_tok.fit_on_texts(train_decoder_input)\n",
    "en_tok.fit_on_texts(train_decoder_label)\n",
    "\n",
    "train_decoder_input = en_tok.texts_to_sequences(train_decoder_input)\n",
    "train_decoder_label = en_tok.texts_to_sequences(train_decoder_label)\n",
    "\n",
    "test_decoder_input = en_tok.texts_to_sequences(test_decoder_input)\n",
    "test_decoder_label = en_tok.texts_to_sequences(test_decoder_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "021fe077-61b2-4c64-885f-1671c202ebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thai\n",
      "mean >>  0.7147007553747821\n",
      "eng\n",
      "mean >>  16.5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"thai\")\n",
    "thai_len = [len(seq) for seq in train_encoder_input]\n",
    "print(\"mean >> \",np.mean(thai_len))\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(thai_len,bins=50)\n",
    "\n",
    "\n",
    "print(\"eng\")\n",
    "en_lens = [len(seq) for seq in train_decoder_input]\n",
    "print(\"mean >> \",np.mean(en_lens))\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(en_lens,bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d0d5fa8a-9b4a-4828-8785-cbcaefb9e992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape\n",
      "(17210, 10)\n",
      "(17210, 20)\n",
      "(17210, 20)\n",
      "\n",
      "\n",
      "test dataset shape\n",
      "(7375, 10)\n",
      "(7375, 20)\n",
      "(7375, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "thai_sequence_size = 10\n",
    "en_sequence_size = 20\n",
    "\n",
    "train_encoder_input = pad_sequences(train_encoder_input,padding='post',truncating='post',maxlen=thai_sequence_size)\n",
    "test_encoder_input = pad_sequences(test_encoder_input,padding='post',truncating='post',maxlen=thai_sequence_size)\n",
    "\n",
    "train_decoder_input = pad_sequences(train_decoder_input,padding='post',truncating='post',maxlen=en_sequence_size)\n",
    "train_decoder_label = pad_sequences(train_decoder_label,padding='post',truncating='post',maxlen=en_sequence_size)\n",
    "\n",
    "test_decoder_input = pad_sequences(test_decoder_input,padding='post',truncating='post',maxlen=en_sequence_size)\n",
    "test_decoder_label = pad_sequences(test_decoder_label,padding='post',truncating='post',maxlen=en_sequence_size)\n",
    "\n",
    "print(\"train dataset shape\")\n",
    "print(train_encoder_input.shape)\n",
    "print(train_decoder_input.shape)\n",
    "print(train_decoder_label.shape)\n",
    "\n",
    "print(\"\\n\\ntest dataset shape\")\n",
    "print(test_encoder_input.shape)\n",
    "print(test_decoder_input.shape)\n",
    "print(test_decoder_label.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fea69814-f08a-42ce-b948-4a0837e1db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Embedding,LSTM,Dense,Concatenate,Attention\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "#hyperparameters\n",
    "embedding_size = 256\n",
    "hidden_size = 256\n",
    "\n",
    "# trainer model (generator model will use the same encoder tho)\n",
    "encoder_input = Input(shape=[thai_sequence_size])\n",
    "encoder_embedding = Embedding(thai_vocab_size,embedding_size,mask_zero=True)\n",
    "encoder_embedded = encoder_embedding(encoder_input)\n",
    "\n",
    "encoder_lstm1 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n",
    "encoder_output1,encoder_h1,encoder_c1 = encoder_lstm1(encoder_embedded)\n",
    "\n",
    "encoder_lstm2 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n",
    "encoder_output2,encoder_h2,encoder_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "encoder_lstm3 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n",
    "encoder_output3,encoder_h3,encoder_c3 = encoder_lstm3(encoder_output1)\n",
    "\n",
    "decoder_input = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(en_vocab_size,embedding_size,mask_zero=True)\n",
    "decoder_embedded = decoder_embedding(decoder_input)\n",
    "\n",
    "decoder_lstm = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n",
    "decoder_output,_,_ = decoder_lstm(decoder_embedded,initial_state=[encoder_h3,encoder_c3])\n",
    "\n",
    "attn_layer = Attention()\n",
    "attn_context = attn_layer([decoder_output,encoder_output3])\n",
    "\n",
    "decoder_output = Concatenate(axis=-1)([decoder_output,attn_context])\n",
    "tanh_dense= Dense(hidden_size*2,activation=K.tanh)\n",
    "decoder_output = tanh_dense(decoder_output)\n",
    "\n",
    "softmax_dense = Dense(en_vocab_size,activation='softmax')\n",
    "decoder_output = softmax_dense(decoder_output)\n",
    "\n",
    "trainer_model = Model([encoder_input,decoder_input],decoder_output)\n",
    "trainer_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "97d12d1e-c3ff-44b5-9f55-7332b78fca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(trainer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf04d774-665a-40cb-a426-41ff845f8ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "108/108 [==============================] - 154s 1s/step - loss: 4.9007 - accuracy: 0.0530 - val_loss: 4.5500 - val_accuracy: 0.0616\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 146s 1s/step - loss: 4.4570 - accuracy: 0.0682 - val_loss: 4.4033 - val_accuracy: 0.0727\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 146s 1s/step - loss: 4.2560 - accuracy: 0.0802 - val_loss: 4.1547 - val_accuracy: 0.0885\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 147s 1s/step - loss: 3.9699 - accuracy: 0.1038 - val_loss: 3.9651 - val_accuracy: 0.1064\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 148s 1s/step - loss: 3.7845 - accuracy: 0.1174 - val_loss: 3.8664 - val_accuracy: 0.1172\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 149s 1s/step - loss: 3.6551 - accuracy: 0.1265 - val_loss: 3.8052 - val_accuracy: 0.1249\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 148s 1s/step - loss: 3.5432 - accuracy: 0.1376 - val_loss: 3.7640 - val_accuracy: 0.1309\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 147s 1s/step - loss: 3.4446 - accuracy: 0.1469 - val_loss: 3.7372 - val_accuracy: 0.1366\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 149s 1s/step - loss: 3.3544 - accuracy: 0.1546 - val_loss: 3.7114 - val_accuracy: 0.1378\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 148s 1s/step - loss: 3.2734 - accuracy: 0.1616 - val_loss: 3.6990 - val_accuracy: 0.1431\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "trainer_hist =trainer_model.fit([train_encoder_input,train_decoder_input],train_decoder_label,epochs=10,batch_size=128,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "69ce951a-1f9b-47ff-8ef1-1100f1541c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_encoder = Model([encoder_input],[encoder_output3,encoder_h3,encoder_c3])\n",
    "\n",
    "gen_decoder_values_input = Input(shape=(thai_sequence_size,hidden_size))\n",
    "gen_decoder_h_input = Input(shape=[hidden_size])\n",
    "gen_decoder_c_input = Input(shape=[hidden_size])\n",
    "\n",
    "gen_decoder_embedded = decoder_embedding(decoder_input)\n",
    "gen_decoder_output,gen_decoder_h,gen_decoder_c = decoder_lstm(gen_decoder_embedded,initial_state=[gen_decoder_h_input,gen_decoder_c_input])\n",
    "\n",
    "attn_context = attn_layer([gen_decoder_output,gen_decoder_values_input])\n",
    "gen_decoder_output = Concatenate(axis=-1)([gen_decoder_output,attn_context])\n",
    "\n",
    "gen_decoder_output = tanh_dense(gen_decoder_output)\n",
    "gen_decoder_output = softmax_dense(gen_decoder_output)\n",
    "\n",
    "gen_decoder = Model([decoder_input]+[gen_decoder_values_input,gen_decoder_h_input,gen_decoder_c_input],[gen_decoder_output,gen_decoder_h,gen_decoder_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "890b0051-19d1-4e0c-a2a2-41d1e1a5b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def seq2eng(seq):\n",
    "    ret =[]\n",
    "    for n in seq:\n",
    "        if n != 0:\n",
    "            ret.append(thai_tok.index_word[n])\n",
    "    ret = ' '.join(ret)\n",
    "    return ret\n",
    "\n",
    "def seq2fr(seq):\n",
    "    ret =[]\n",
    "    for n in seq:\n",
    "        if n != 0 and en_tok.index_word[n] != 'eostoken':\n",
    "            ret.append(en_tok.index_word[n])\n",
    "    ret = ' '.join(ret)\n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6939af0e-57a8-4b77-93c5-25f748bb3291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_from_encoder_input(encoder_input):\n",
    "    encoder_input = encoder_input.reshape(1,-1)\n",
    "    values,h,c = gen_encoder.predict(encoder_input)\n",
    "    \n",
    "    single_tok = np.zeros((1,1))\n",
    "    single_tok[0,0] = en_tok.word_index['sostoken']\n",
    "    decoder_input = single_tok\n",
    "    \n",
    "    generated = []\n",
    "    count = 0\n",
    "    while(True):\n",
    "        decoder_output,new_h,new_c = gen_decoder.predict([decoder_input]+[values,h,c])\n",
    "        count +=1\n",
    "        \n",
    "        sampled_index = np.argmax(decoder_output[0,-1,:])\n",
    "        sampled_word = en_tok.index_word[sampled_index]\n",
    "        \n",
    "        if sampled_word != 'eostoken' and sampled_index != 0:\n",
    "            generated.append(sampled_word)\n",
    "        if count >= en_sequence_size or sampled_word == 'eostoken':\n",
    "            break\n",
    "        \n",
    "        h,c = new_h,new_c\n",
    "        decoder_input[0,0] = sampled_index\n",
    "    \n",
    "    generated = ' '.join(generated)\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f62a73c-3f69-4e88-8fab-2f7bed1b3a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<sample encoder input english sentence>>\n",
      "ฉันต้องการเคสกันน้ำสำหรับกล้องเพราะฉันมักจะโดนพายุและ หรือวิ่งเข้าไปในป่าดังนั้นสิ่งเหล่านี้ใช้ได้กับฉันตราบใดที่พวกเขายังคงปิดอยู่\n",
      "\n",
      "\n",
      "<<sample generated french sentence>>\n",
      "1/1 [==============================] - 0s 411ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "the author has been introduced to be a broad 54 85\n",
      "\n",
      "\n",
      "<<answer french sentence>>\n",
      "i wanted water proof cases for the cameras because i'm prone to getting caught in thunderstorms and or running off\n",
      "========================================\n",
      "\n",
      "\n",
      "<<sample encoder input english sentence>>\n",
      "หากคุณต้องการดูว่าดาวเหล่านี้กำลังทำอะไรในช่วงเวลานั้นเช่นเดียวกับการได้เห็นรูปลักษณ์ที่สวยงามและอ่อนเยาว์ของพวกเขาคุณอาจจะสนุกไปกับมัน\n",
      "\n",
      "\n",
      "<<sample generated french sentence>>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "i have to be a fan of the book and the book was a little bit and the book was\n",
      "\n",
      "\n",
      "<<answer french sentence>>\n",
      "if you want to see what all of those stars were doing at that time period as well as seeing\n",
      "========================================\n",
      "\n",
      "\n",
      "<<sample encoder input english sentence>>\n",
      "ในขณะที่ฉันรักซอมบี้ที่ดีจริงๆ คัมภีร์ของศาสนาคริสต์ที่เปิดเผยสำหรับหนังสือที่น่าพิศวงไม่มีอะไรแปลกใหม่มันค่อนข้างน่าเบื่อและไม่เป็นต้นฉบับ\n",
      "\n",
      "\n",
      "<<sample generated french sentence>>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "if you want to be sure how much like it is a good read and i have read it\n",
      "\n",
      "\n",
      "<<answer french sentence>>\n",
      "while i love a really good zombie apocalyptic apocalypse that would make for an amazing book there's nothing too original\n",
      "========================================\n",
      "\n",
      "\n",
      "<<sample encoder input english sentence>>\n",
      "ฉันไม่เคยไปนิวออร์ลีนส์ แต่หนังสือเล่มนี้ได้ให้ความเข้าใจที่ดีขึ้นในสิ่งที่ทำให้เมืองถูกติ๊ก\n",
      "\n",
      "\n",
      "<<sample generated french sentence>>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "i have to read the book and the book was a little bit and the book was a little bit\n",
      "\n",
      "\n",
      "<<answer french sentence>>\n",
      "i have never been to new orleans but this book has given me a better understanding of what makes the\n",
      "========================================\n",
      "\n",
      "\n",
      "<<sample encoder input english sentence>>\n",
      "มีผลิตภัณ์ที่ดีกว่า ถ้าคุณตะหลีกเลี่ยงสารตกค้างอลูมิเนียม\n",
      "\n",
      "\n",
      "<<sample generated french sentence>>\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "if you want to be looking for any other words or any other words\n",
      "\n",
      "\n",
      "<<answer french sentence>>\n",
      "there are better products out there if you're looking to avoid aluminum residue\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(520,525):\n",
    "    print(\"\\n<<sample encoder input english sentence>>\")\n",
    "    print(seq2eng(train_encoder_input[i]))\n",
    "    print(\"\\n\")\n",
    "    print(\"<<sample generated french sentence>>\")\n",
    "    print(generate_from_encoder_input(train_encoder_input[i]))\n",
    "    print(\"\\n\")\n",
    "    print(\"<<answer french sentence>>\")\n",
    "    print(seq2fr(train_decoder_label[i]))\n",
    "    print(\"========================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "833d962a-dd2b-4c1b-be1e-b71f7bb177f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on Train Dataset\n",
      "\n",
      "INPUT ENG>>\n",
      "ลูกสาวของฉันรักมัน มันทำออกมาได้ดีมากๆ\n",
      "\n",
      "\n",
      "GENERATED FR>>\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "i bought this book to be a great book but it was a great book but it was a great\n",
      "\n",
      "\n",
      "ANSWER FR>>\n",
      "my daughter loved it it's very well made\n",
      "=====================================================================\n",
      "\n",
      "\n",
      "INPUT ENG>>\n",
      "มีขุยใต้ทั้งสองด้าน ผลิตภัณฑ์นี้ครับ\n",
      "\n",
      "\n",
      "GENERATED FR>>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "i bought this book and the first time i have been a little bit and the first time and it\n",
      "\n",
      "\n",
      "ANSWER FR>>\n",
      "there was lint under both sides of it this product sucks\n",
      "=====================================================================\n",
      "\n",
      "\n",
      "INPUT ENG>>\n",
      "เราต้องกลับไปและลองสิ่งที่ยากขึ้นในวันถัดไป\n",
      "\n",
      "\n",
      "GENERATED FR>>\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "i bought this product and the first time\n",
      "\n",
      "\n",
      "ANSWER FR>>\n",
      "we had to go back and try some more difficult ones the next day\n",
      "=====================================================================\n",
      "\n",
      "\n",
      "INPUT ENG>>\n",
      "แม้ราคา 2 ต่อเกมส์ มันก็จะเกินราคา\n",
      "\n",
      "\n",
      "GENERATED FR>>\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "i bought this product for my ipad 3 months\n",
      "\n",
      "\n",
      "ANSWER FR>>\n",
      "even at 2 per game it would be overpriced\n",
      "=====================================================================\n",
      "\n",
      "\n",
      "INPUT ENG>>\n",
      "ฉันอ่านหนังสือของแจ๊ค แดเนียลส์สองเล่มแรกแล้ว และรอคอยเรื่องต่อของพวกเขาอยู่\n",
      "\n",
      "\n",
      "GENERATED FR>>\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "i have read a little read and the book was a little read and the book was a little bit\n",
      "\n",
      "\n",
      "ANSWER FR>>\n",
      "i read the first two jack daniels books and was looking forward to a continuation of their story\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = [24,302, 20, 1, 55]\n",
    "\n",
    "print(\"Results on Train Dataset\")\n",
    "for i in idx:\n",
    "    print(\"\\nINPUT ENG>>\")\n",
    "    print(seq2eng(train_encoder_input[i]))\n",
    "    print(\"\\n\")\n",
    "    print(\"GENERATED FR>>\")\n",
    "    print(generate_from_encoder_input(train_encoder_input[i]))\n",
    "    print(\"\\n\")\n",
    "    print(\"ANSWER FR>>\")\n",
    "    print(seq2fr(train_decoder_label[i]))\n",
    "    print(\"=====================================================================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
