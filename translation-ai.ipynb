{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8432f536-7082-44d3-89c5-4501ff465d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.23.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d63ad5f7-2eba-4ca1-b2b5-b558957f4c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sklearn) (1.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.23.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (58.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "898ec549-7e35-418a-a1fa-530da476e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6efc9-ffaf-4d25-9308-0d69b949dd84",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13f40438-cc4b-45b9-9caa-cd65a12c8dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_text</th>\n",
       "      <th>th_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doesn't snap together well.</td>\n",
       "      <td>เข้ากันไม่ค่อยดี</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charged it after every use as directed for abo...</td>\n",
       "      <td>เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My son wanted this movie so badly, that he sai...</td>\n",
       "      <td>ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But his writing has degenerated in later books.</td>\n",
       "      <td>แต่หนังสือเล่มที่ผ่านมาของเขามันดูด้อยคุณภาพลง</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was supposed to be a good bag, well you get...</td>\n",
       "      <td>มันควรที่จะเป็นกระเป๋าที่ดี เอ้อ เธอได้ในสิ่งท...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_text  \\\n",
       "0                        Doesn't snap together well.   \n",
       "1  Charged it after every use as directed for abo...   \n",
       "2  My son wanted this movie so badly, that he sai...   \n",
       "3    But his writing has degenerated in later books.   \n",
       "4  It was supposed to be a good bag, well you get...   \n",
       "\n",
       "                                             th_text  \n",
       "0                                   เข้ากันไม่ค่อยดี  \n",
       "1  เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไ...  \n",
       "2  ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี...  \n",
       "3     แต่หนังสือเล่มที่ผ่านมาของเขามันดูด้อยคุณภาพลง  \n",
       "4  มันควรที่จะเป็นกระเป๋าที่ดี เอ้อ เธอได้ในสิ่งท...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('translation-dataset/generated_reviews_crowd.csv')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bc5c7982-3488-4ccd-b72e-684f82071a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24587, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_text</th>\n",
       "      <th>th_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doesn't snap together well.</td>\n",
       "      <td>เข้ากันไม่ค่อยดี</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charged it after every use as directed for abo...</td>\n",
       "      <td>เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My son wanted this movie so badly, that he sai...</td>\n",
       "      <td>ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_text  \\\n",
       "0                        Doesn't snap together well.   \n",
       "1  Charged it after every use as directed for abo...   \n",
       "2  My son wanted this movie so badly, that he sai...   \n",
       "\n",
       "                                             th_text  \n",
       "0                                   เข้ากันไม่ค่อยดี  \n",
       "1  เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไ...  \n",
       "2  ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e6b77187-f36d-43b5-9b64-93c190bafa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "from string import punctuation\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_string(string):\n",
    "    # Replace no-break space with space\n",
    "    string = string.replace(\"\\u202f\",\" \")\n",
    "    # Converts all uppercase characters into lowercase characters\n",
    "    string = string.lower()\n",
    "\n",
    "    # Delete the punctuation and the numbers\n",
    "    for p in punctuation + \"«»\" + \"0123456789\":\n",
    "        string = string.replace(p,\" \")\n",
    "\n",
    "    # Eliminate duplicate whitespaces using wildcards   \n",
    "    string = re.sub(\"\\s+\",\" \", string)\n",
    "    # Remove spaces at the beginning and at the end of the string\n",
    "    string = string.strip()\n",
    "           \n",
    "    return string\n",
    "dataset[\"en_text\"] = dataset[\"en_text\"].astype(str)\n",
    "dataset[\"th_text\"]  = dataset[\"th_text\"].astype(str)\n",
    "\n",
    "dataset[\"en_text\"] = dataset[\"en_text\"].apply(lambda x: clean_string(x))\n",
    "dataset[\"th_text\"]  = dataset[\"th_text\"].apply(lambda x: clean_string(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b905a97a-d704-425f-91d6-e1659bf4ab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] will stop using since can t afford doing every time its full [end]\n",
      "[start] i have found that this book is rather weak when it comes to explaining why the authors recommend certain tips [end]\n",
      "[start] the product they sent me didn t even look like what you see here and certainly wasn t the item pictured on amazon [end]\n",
      "[start] most obvious was that you need an awful lot of knowledge or understanding about how people think on a daily basis at least if you re going for plausible fiction [end]\n",
      "[start] have had these now about months [end]\n",
      "\n",
      "---------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('เข้ากันไม่ค่อยดี', '[start] doesn t snap together well [end]'),\n",
       " ('เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไว้เป็นเวลาประมาณ เดือนเริ่มมีปัญหาในการเก็บประจุ',\n",
       "  '[start] charged it after every use as directed for about months started having difficulty holding a charge on it [end]'),\n",
       " ('ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี ที่เขากล่าวว่ามันอย่างน้อย ครั้ง',\n",
       "  '[start] my son wanted this movie so badly that he said it at least times [end]'),\n",
       " ('แต่หนังสือเล่มที่ผ่านมาของเขามันดูด้อยคุณภาพลง',\n",
       "  '[start] but his writing has degenerated in later books [end]'),\n",
       " ('มันควรที่จะเป็นกระเป๋าที่ดี เอ้อ เธอได้ในสิ่งที่เธอจ่ายไป',\n",
       "  '[start] it was supposed to be a good bag well you get what you pay for [end]')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_en = dataset[\"en_text\"].values\n",
    "raw_data_th = dataset[\"th_text\"].values\n",
    "\n",
    "# Add start and end\n",
    "raw_data_en_in_out = [\"[start] \" + st + \" [end]\" for st in raw_data_en]\n",
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(raw_data_en_in_out))\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------\")\n",
    "# Each line contains an English sentence and its corresponding French sentence. \n",
    "# The English sentence is the source sequence and the French one is the target sequence.\n",
    "my_data = []\n",
    "for i in range(len(raw_data_th)):\n",
    "    th = raw_data_th[i]\n",
    "    en = raw_data_en_in_out[i]\n",
    "    my_data.append((th, en))\n",
    "    \n",
    "my_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4f3e97a6-f6c1-4419-b756-e4bc1d31cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs in my dtat :  24587\n",
      "Training pairs         :  19671\n",
      "Validation pairs       :  2458\n",
      "Test pairs             :  2458\n"
     ]
    }
   ],
   "source": [
    "# Shuffle dataset\n",
    "\n",
    "random.shuffle(my_data)\n",
    "\n",
    "# Train_set = 80%, Test_set = 10%, Val_set = 10%\n",
    "num_val_samples = int(0.1 * len(my_data))\n",
    "num_train_samples = len(my_data) - 2 * num_val_samples\n",
    "\n",
    "train_pairs = my_data[:num_train_samples]\n",
    "val_pairs   = my_data[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs  = my_data[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(\"Total pairs in my dtat : \",len(my_data))\n",
    "print(\"Training pairs         : \",len(train_pairs))\n",
    "print(\"Validation pairs       : \",len(val_pairs))\n",
    "print(\"Test pairs             : \",len(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2c26f4ab-8ef2-4951-bba3-69eb6adf7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6bf29bd8-45dc-42d9-a168-97cd0ee4b97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Vectorizing the text data\n",
    "vocab_size      = 15000\n",
    "sequence_length = 25\n",
    "batch_size      = 64\n",
    "\n",
    "en_vectorization = TextVectorization(\n",
    "    max_tokens = vocab_size, output_mode = \"int\", \n",
    "    output_sequence_length = sequence_length + 1,)\n",
    "\n",
    "th_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,)\n",
    "\n",
    "train_th_texts = [pair[0] for pair in train_pairs]\n",
    "train_en_texts = [pair[1] for pair in train_pairs]\n",
    "\n",
    "en_vectorization.adapt(train_en_texts)\n",
    "th_vectorization.adapt(train_th_texts)\n",
    " \n",
    "print(\"Done ...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "63ac4b87-0aa4-4552-9570-cb096e3f4e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 25)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 25)\n",
      "targets.shape: (64, 25)\n"
     ]
    }
   ],
   "source": [
    "def format_dataset(en, th):\n",
    "    th = th_vectorization(th)\n",
    "    en = en_vectorization(en)\n",
    "    return ({\"encoder_inputs\": th, \"decoder_inputs\": en[:, :-1],}, en[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    th_texts, en_texts = zip(*pairs)\n",
    "\n",
    "    th_texts = list(th_texts)\n",
    "    en_texts = list(en_texts)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((en_texts, th_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(1024).prefetch(32).cache()\n",
    "\n",
    "\n",
    "train_dataset = make_dataset(train_pairs)\n",
    "val_dataset   = make_dataset(val_pairs)\n",
    "\n",
    "for inputs, targets in train_dataset.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a7af35b1-5275-4913-af75-090c7d14dc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ...\n"
     ]
    }
   ],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "         #-----------------------------------------------------------------------\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size      = vocab_size\n",
    "        self.embed_dim       = embed_dim\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # L3\n",
    "        self.input_embeddings = layers.Embedding(\n",
    "                                     input_dim=vocab_size, output_dim=embed_dim)\n",
    "        \n",
    "        #-----------------------------------------------------------------------\n",
    "        # L4\n",
    "        self.positional_encoding = layers.Embedding(\n",
    "                                input_dim=sequence_length, output_dim=embed_dim)\n",
    "       \n",
    "    #---------------------------------------------------------------------------\n",
    "    def call(self, inputs):\n",
    "        length    = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "\n",
    "        # L3\n",
    "        embedded_inputs    = self.input_embeddings(inputs)\n",
    "        # L4\n",
    "        embedded_positions = self.positional_encoding(positions)\n",
    "        # L5\n",
    "        return embedded_inputs + embedded_positions\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "        \n",
    "#*******************************************************************************\n",
    "print(\"Done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b64eb429-692b-47dc-b914-68733b821141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ...\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        #-----------------------------------------------------------------------\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        # L6\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "                                         num_heads=num_heads, key_dim=embed_dim)\n",
    "        # L8\n",
    "        self.feed_forward = keras.Sequential([\n",
    "                                  layers.Dense(dense_dim, activation=\"relu\"), \n",
    "                                   layers.Dense(embed_dim),])\n",
    "        \n",
    "        # L7\n",
    "        self.normalization_1  = layers.LayerNormalization()\n",
    "        # L9\n",
    "        self.normalization_2  = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # L6\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask)\n",
    "        \n",
    "        #-----------------------------------------------------------------------\n",
    "        # L7\n",
    "        addnorm_output_1 = self.normalization_1(inputs + attention_output)\n",
    "        # L8\n",
    "        feedforward_output = self.feed_forward(addnorm_output_1)\n",
    "        # L9\n",
    "        addnorm_output_2 = self.normalization_2(addnorm_output_1 + feedforward_output)\n",
    "        # L10\n",
    "        return addnorm_output_2\n",
    "\n",
    "#*******************************************************************************\n",
    "print(\"Done ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e04d44c6-b800-4696-b001-325a31d4fa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        #-----------------------------------------------------------------------\n",
    "        self.embed_dim  = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads  = num_heads\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # L11\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "                                         num_heads=num_heads, key_dim=embed_dim)\n",
    "        # L13\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "                                         num_heads=num_heads, key_dim=embed_dim)\n",
    "        #-----------------------------------------------------------------------\n",
    "        # L15\n",
    "        self.feed_forward = keras.Sequential([\n",
    "          layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
    "        \n",
    "        # L12\n",
    "        self.normalization_1  = layers.LayerNormalization()\n",
    "        # L14\n",
    "        self.normalization_2  = layers.LayerNormalization()\n",
    "        # L16\n",
    "        self.normalization_3  = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # L11\n",
    "        attention_output_1 = self.attention_1(\n",
    "             query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
    "        # L12\n",
    "        addnorm_output_1 = self.normalization_1(inputs + attention_output_1)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # L13\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=addnorm_output_1, value=encoder_outputs, key=encoder_outputs,\n",
    "            attention_mask=padding_mask,)\n",
    "        # L14\n",
    "        addnorm_output_2 = self.normalization_2(addnorm_output_1 + attention_output_2)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # L15\n",
    "        feedforward_output = self.feed_forward(addnorm_output_2)\n",
    "        # L16\n",
    "        addnorm_output_3 = self.normalization_3(addnorm_output_2 + feedforward_output)\n",
    "\n",
    "        return addnorm_output_3\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "        \n",
    "#*******************************************************************************\n",
    "print(\"Done ...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a387a64-8677-4b51-849e-8343823844b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: graphviz in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.20)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydot) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3df30cbe-6174-45c2-9372-fe6704640f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_12 (Posit  (None, None, 128)   1923200     ['encoder_inputs[0][0]']         \n",
      " ionalEmbedding)                                                                                  \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_6 (Transfo  (None, None, 128)   923136      ['positional_embedding_12[0][0]']\n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " model_13 (Functional)          (None, None, 15000)  5440920     ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder_6[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,287,256\n",
      "Trainable params: 8,287,256\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "embed_dim  = 128\n",
    "latent_dim = 1024\n",
    "num_heads  = 10\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# L1\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "# L3, L4, L5\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# From L6 to L10\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "# Encoder\n",
    "encoder         = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# L2\n",
    "decoder_inputs     = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "# L10\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "# L3, L4, L5\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "# From L11 to L16\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Output Probabilities\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Decoder\n",
    "decoder         = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# My Transformer\n",
    "my_transformer = keras.Model(\n",
    "                          [encoder_inputs, decoder_inputs], \n",
    "                          decoder_outputs, name=\"my_transformer\")\n",
    "\n",
    "# The model’s summary() method displays all the model’s layers\n",
    "print(my_transformer.summary())\n",
    "\n",
    "# Plot the model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(my_transformer, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8f1c0aec-cbed-4c00-8983-5b49a7d989a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jamilur\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->tensorflow-addons) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jamilur\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75cac5-6943-42dd-95f5-f54a5a47c644",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "308/308 [==============================] - 228s 732ms/step - loss: 3.6995 - accuracy: 0.1497 - val_loss: 3.2109 - val_accuracy: 0.1941\n",
      "Epoch 2/20\n",
      "308/308 [==============================] - 222s 720ms/step - loss: 3.1532 - accuracy: 0.2013 - val_loss: 3.0709 - val_accuracy: 0.2074\n",
      "Epoch 3/20\n",
      "308/308 [==============================] - 224s 729ms/step - loss: 2.9937 - accuracy: 0.2171 - val_loss: 3.0117 - val_accuracy: 0.2128\n",
      "Epoch 4/20\n",
      "308/308 [==============================] - 222s 722ms/step - loss: 2.8742 - accuracy: 0.2309 - val_loss: 2.9798 - val_accuracy: 0.2136\n",
      "Epoch 5/20\n",
      "308/308 [==============================] - 222s 720ms/step - loss: 2.7741 - accuracy: 0.2435 - val_loss: 2.9597 - val_accuracy: 0.2203\n",
      "Epoch 6/20\n",
      "308/308 [==============================] - 223s 724ms/step - loss: 2.6857 - accuracy: 0.2554 - val_loss: 2.9750 - val_accuracy: 0.2218\n",
      "Epoch 7/20\n",
      "308/308 [==============================] - 255s 829ms/step - loss: 2.6067 - accuracy: 0.2672 - val_loss: 2.9649 - val_accuracy: 0.2183\n",
      "Epoch 8/20\n",
      "308/308 [==============================] - 255s 827ms/step - loss: 2.5354 - accuracy: 0.2773 - val_loss: 3.0026 - val_accuracy: 0.2195\n",
      "Epoch 9/20\n",
      "308/308 [==============================] - 255s 828ms/step - loss: 2.4655 - accuracy: 0.2885 - val_loss: 3.0310 - val_accuracy: 0.2214\n",
      "Epoch 10/20\n",
      "308/308 [==============================] - 248s 805ms/step - loss: 2.4003 - accuracy: 0.2987 - val_loss: 3.0133 - val_accuracy: 0.2234\n",
      "Epoch 11/20\n",
      "308/308 [==============================] - 238s 772ms/step - loss: 2.3398 - accuracy: 0.3104 - val_loss: 3.0034 - val_accuracy: 0.2248\n",
      "Epoch 12/20\n",
      "308/308 [==============================] - 252s 819ms/step - loss: 2.2855 - accuracy: 0.3201 - val_loss: 3.0353 - val_accuracy: 0.2196\n",
      "Epoch 13/20\n",
      "308/308 [==============================] - 243s 790ms/step - loss: 2.2293 - accuracy: 0.3314 - val_loss: 3.0705 - val_accuracy: 0.2158\n",
      "Epoch 14/20\n",
      " 72/308 [======>.......................] - ETA: 2:49 - loss: 2.2061 - accuracy: 0.3361"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate = 0.001, weight_decay=0.0001)\n",
    "\n",
    "# Compiling the model\n",
    "my_transformer.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                       optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "\n",
    "# Training the model \n",
    "epochs = 20 # You have to train it longer to converge\n",
    "history = my_transformer.fit(train_dataset, epochs=epochs, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ef744-0665-4628-a1d0-a6ba696bb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history.history).plot(figsize=(12, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "test_dataset   = make_dataset(test_pairs)\n",
    "model_evaluate = my_transformer.evaluate(test_dataset)\n",
    "print(\"Loss     : \",model_evaluate[0])\n",
    "print(\"accuracy : \",model_evaluate[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6b79a-deeb-488e-8782-2fbe9a877933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "en_vocab = en_vectorization.get_vocabulary()\n",
    "en_index_lookup = dict(zip(range(len(en_vocab)), en_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = th_vectorization([input_sentence])\n",
    "\n",
    "    decoded_sentence = \"[start]\"\n",
    "\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = en_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = my_transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = en_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "test_th_texts = [pair[0] for pair in test_pairs]\n",
    "exp = 0\n",
    "for i in range(30):\n",
    "    input_sentence = random.choice(test_th_texts)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(\"Example : \",exp)\n",
    "    print(\"TH: \",test_th_texts[i])\n",
    "\n",
    "    translated = translated.replace(\"[start]\", \"\")\n",
    "    translated = translated.replace(\"[UNK]\", \"\")\n",
    "    translated = translated.replace(\"end\", \"\")\n",
    "    print(\"EN: \",translated)\n",
    "    exp = exp + 1\n",
    "    print(\"-------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada86d8-2a8f-4407-b412-7c56aa0c7e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
